---
title: "Introduction to applied Machine Learning - Terminology and Workflow"
author: "Johannes J. Mueller"
date: "13 September 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We launched our Machine Learning for Social Good program! Our goal is to democratice Machine Learning and make it more accesible to people who love coding but who haven't tried their hands it yet! In our challenges you can experiment with real-life data and get started by building your own models! 

To make your entry to the ML world easier we have prepared some introductory blog posts where we explain the basic concepts. We furthermore provide example code in R and Python which will guide you through the first two challenges. 

## What is Machine Learning?

Machine Learning is a broad field that brings together very different techniques and objectives. In our Machine Learning Challenges you will encounter different problems. However, they all share the same approach: Building a **Classifier** using **Supervised Machine Learning**.

In **Classification** the outcome variable is categorical. We are trying to predict for example "*peace agreement fails* vs. *peace agreement holds*" or "*person donates blood again* vs. *person will not donate blood again*". The alternative is "Regression" which we use when the outcome variable is metric.

**Supervised Machine Learning** means that we know the categories of observations from the start. In our example we know that there are two types of peace agreements (those who failed and didn't). The alternative is called unsupervised learning where we let the algorithm define the groups in the data itself. 

For a quick overview over different types of Machine Learning read this:

https://medium.com/deep-math-machine-learning-ai/different-types-of-machine-learning-and-their-types-34760b9128a2

https://www.kdnuggets.com/2017/11/3-different-types-machine-learning.html

## Key Terminology

When reading about ML approaches it is important to know a few key terms. In the following we only list the absolute fundamentals. 

  * **Label**: This is the variable we want to predict. In classification the label marks to which category an observation belongs to. In the social sciences this variable would be called the "dependent variable". 
  * **Feature**: Features are all the variables we have to model the variance in our outcome. In the social sciences features would be called "independent variables".
  * **Model**: Our model is a function between the features and the outcome. The goal is to create a model that can look at new observations and then give them the correct label (aka. classifying them correctly).There are different kind of models that are used in supervised machine learning. They range from logistic regression to deep neural networks. For an overview of different models and approaches look at: https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
  * **loss function**: When we build a model we can quantify how well the model classifies the observations. This is important so that we can actually find "the best model". The loss function is very similiar to the concept of log-likelihood as used in the social sciences. 
  * **Learning**: The "learning" part in Machine Learning means that we use an algorithm to find the best model. The alorithm does so by minimizing the loss function. The most common algorithm is called "gradient descent" (https://hackernoon.com/gradient-descent-aynk-7cbe95a778da).
  * **Training dataset**: To build our model we use the so called training dataset. The traning data contains all features as well as the labels. 
  * **Test dataset**: Normally we want to assess the performance of our model not on the same data that we trained it on. Therefore we create a subset of the data that is only used for evaluating the model. If you're interested in why we don't evaluate our model with the data we trained it on, read this article on *overfitting* (https://elitedatascience.com/overfitting-in-machine-learning)
  
For an exhasutive list look at this Glossary:  https://developers.google.com/machine-learning/glossary/

## Workflow

Now that we know the key terminology we can start building our first model. The workflow for building ML classifiers always follows the same basic workflow:

### 1. Loading the data and dependencies 

For a basic setup I load *ggplot2* for exploratory data analysis, and *caret* for the machine learning part. *caret* is a very powerful package in R in which all the major ML algorithms and models are already implemented. 

```{r packages, warning=FALSE, message=FALSE}
library(caret)
library(ggplot2)

#download the peace_train.csv file from Kaggle
data <- read.csv("peace_train.csv")
head(data, 4)
```

If we take a look at the data we see the label *ended* as well as all the features. 

### 2. Explore the data

In the next step we usually get to know the data a bit better. We look at the distribution of the outcome variable as well as the different features. Questions that you might be interested are:

  * How many features do we have?
  * How are they scaled? 
  * Do features correlate with each other? 

At this point it might be necessary to pre-process the data. If you're interested in different ways to do so check out: https://machinelearningmastery.com/pre-process-your-dataset-in-r/

For this quick runthrough we will just delete the unnecessary "Id" variable.

```{r processing}
data$Id <- NULL
data$X <- NULL
```
### 3. Split into training and test dataset
No that we have loaded the data we will split it into training and test dataset. We will use the *createDataPartition* function from the caret package. It has two important arguments: the outcome variable (which will ensure that the distribution of outcome will be similiar in both groups) and *p*, what proportion of the data should go into the training dataset. 

```{r split}
Index <- createDataPartition(data$ended, p=0.80, list=FALSE)
data_train <- data[Index,] # delete ID var
data_test <- data[-Index,] # delete ID var
```

### 4. Train the model

Now it is time to train the model. We use the *train* function. It takes a *forumla* as the first argument. This works very similar to specifying a regression in R. With the **.** we specify that we use all available features in the dataset (alternatively you can specify them as you would in a regression *y~x1+ x2 + x3+...*).

With the method argument you specify which model you want to use. In this case I am using a simple logistic regression (Spoiler-alert: not the best model for this case). There are dozens of models you can specify here. For an overview look at: https://rdrr.io/cran/caret/man/models.html. Lastly, we specify which metric we use for optimisation. The default value is "Accuracy" but have a look at other options here: https://topepo.github.io/caret/model-training-and-tuning.html#metrics . 

As we are just using a log. regression here we can also call the summary function to see the individual coefficients for each feature. 

```{r train, warning=FALSE, message=FALSE}
# train the model
fit.glm <- train(as.factor(ended) ~ .,
                 data=data_train, method="glm", metric="Accuracy")

summary(fit.glm)
```

### 5. Evaluation

After we trained our model it is time to evaluate how well it predicts new cases. Therefore we predict the labels for our test dataset and construct a confusion matrix which shows how the predictions fair against the true labels. One obvious metric we are interested in is the "accuracy". A guide to the other metrics can be found here: https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/

```{r evaluation}
predictions <- predict(fit.glm, newdata = data_test)
confusionMatrix(predictions, data_test$ended)
```

### 6. Uploading your solution to Kaggle

Now that you have build a (hopefully) great model you can upload it to Kaggle and see how well your model compares to others.

To do so you have to download the evaluation dataset from Kaggle called "peace_eval_features.csv". In this dataset only the features are given. For submission you have to calculate the predicted probabilities and upload them with their ID.

```{r upload}
peace_eval <- read.csv("peace_eval_features.csv")
predicted_probabilities <- predict(fit.glm, newdata = peace_eval[,1:28], type = "prob")[2]

submission <- as.data.frame(cbind(peace_eval$Id,
                                  predicted_probabilities))
names(submission) <- c("Id", "prob")
# write.csv(submission, file = "submission.csv",row.names = F)

```



